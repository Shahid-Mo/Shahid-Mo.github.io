<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Test | TensorTunes</title>
<meta name="keywords" content="">
<meta name="description" content="In this post, i wont be discussing code implementations, my goal is to cover the foundational concepts related to multi-GPU Training of Massive llms,
as stated in my post on Qunatization, you would need a cluster of gpus just to get up and running with the finetuning of of even small llms like the llama 7B models.
The topics i would like to cover are as follows
DDP (Distributed Data Parallel) Tensor Model parallelism Pipeline model parallelism Memory efficient pipeline parallelism Where Did all the memory go?">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/draft_quant/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/draft_quant/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="TensorTunes (Alt + H)">TensorTunes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Test
    </h1>
    <div class="post-meta"><span title='2024-10-16 00:00:00 +0000 UTC'>October 16, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#where-did-all-the-memory-go" aria-label="Where Did all the memory go?">Where Did all the memory go?</a><ul>
                        
                <li>
                    <a href="#adam-optimizer" aria-label="Adam optimizer">Adam optimizer</a></li>
                <li>
                    <a href="#1-recap-gradient-descent-and-sgd" aria-label="1. Recap: Gradient Descent and SGD">1. Recap: Gradient Descent and SGD</a></li>
                <li>
                    <a href="#2-introducing-adam-optimizer" aria-label="2. Introducing Adam Optimizer">2. Introducing Adam Optimizer</a><ul>
                        
                <li>
                    <a href="#key-concepts" aria-label="Key Concepts:">Key Concepts:</a></li>
                <li>
                    <a href="#adams-update-mechanism" aria-label="Adam&rsquo;s Update Mechanism:">Adam&rsquo;s Update Mechanism:</a></li>
                <li>
                    <a href="#algorithm-summary" aria-label="Algorithm Summary:">Algorithm Summary:</a></li></ul>
                </li>
                <li>
                    <a href="#bf16optimizer" aria-label="BF16Optimizer">BF16Optimizer</a></li></ul>
                </li>
                <li>
                    <a href="#ddp-distributed-data-parallel" aria-label="DDP (Distributed Data Parallel)">DDP (Distributed Data Parallel)</a></li>
                <li>
                    <a href="#pipeline-model-parallelism" aria-label="Pipeline model parallelism">Pipeline model parallelism</a><ul>
                        
                <li>
                    <a href="#background" aria-label="Background">Background</a></li>
                <li>
                    <a href="#what-is-pipeline-parallelism" aria-label="What is Pipeline Parallelism?">What is Pipeline Parallelism?</a></li>
                <li>
                    <a href="#how-pipeline-parallelism-works" aria-label="How Pipeline Parallelism Works">How Pipeline Parallelism Works</a></li>
                <li>
                    <a href="#advantages-of-pipeline-parallelism" aria-label="Advantages of Pipeline Parallelism">Advantages of Pipeline Parallelism</a></li>
                <li>
                    <a href="#challenges-and-solutions-in-pipedream" aria-label="Challenges and Solutions in PipeDream">Challenges and Solutions in PipeDream</a><ul>
                        
                <li>
                    <a href="#challenge-1-work-partitioning" aria-label="Challenge 1: Work Partitioning">Challenge 1: Work Partitioning</a></li>
                <li>
                    <a href="#challenge-2-work-scheduling" aria-label="Challenge 2: Work Scheduling">Challenge 2: Work Scheduling</a></li>
                <li>
                    <a href="#challenge-3-effective-learning" aria-label="Challenge 3: Effective Learning">Challenge 3: Effective Learning</a></li></ul>
                </li>
                <li>
                    <a href="#understanding-staleness-and-consistency" aria-label="Understanding Staleness and Consistency">Understanding Staleness and Consistency</a></li>
                <li>
                    <a href="#memory-considerations" aria-label="Memory Considerations">Memory Considerations</a></li>
                <li>
                    <a href="#implementation-highlights" aria-label="Implementation Highlights">Implementation Highlights</a></li>
                <li>
                    <a href="#benefits-of-pipedreams-pipeline-parallelism" aria-label="Benefits of PipeDream&rsquo;s Pipeline Parallelism">Benefits of PipeDream&rsquo;s Pipeline Parallelism</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a></li></ul>
                </li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In this post, i wont be discussing code implementations, my goal is to cover the foundational concepts related to multi-GPU Training of Massive llms,</p>
<p>as stated in my post on Qunatization, you would need a cluster of gpus just to get up and running with the finetuning of of even small llms like the llama 7B models.</p>
<p>The topics i would like to cover are as follows</p>
<ol>
<li>DDP (Distributed Data Parallel)</li>
<li>Tensor Model parallelism</li>
<li>Pipeline model parallelism</li>
<li>Memory efficient pipeline parallelism</li>
</ol>
<h2 id="where-did-all-the-memory-go">Where Did all the memory go?<a hidden class="anchor" aria-hidden="true" href="#where-did-all-the-memory-go">#</a></h2>
<p>I have eleded to this point already in my qunatization post, but i would like here just to scratch a bit deeper,</p>
<p>to understand this, first we will take a quick look at Adam optimizer, (one of the most popular for training Neural Networks)</p>
<h3 id="adam-optimizer">Adam optimizer<a hidden class="anchor" aria-hidden="true" href="#adam-optimizer">#</a></h3>
<p>(if you are familiar with the adam optimizer you can skip this section)</p>
<h3 id="1-recap-gradient-descent-and-sgd"><strong>1. Recap: Gradient Descent and SGD</strong><a hidden class="anchor" aria-hidden="true" href="#1-recap-gradient-descent-and-sgd">#</a></h3>
<p>Before diving into Adam, let&rsquo;s briefly recap the foundational optimizer:</p>
<ul>
<li><strong>Gradient Descent (GD):</strong></li>
<li><strong>Objective:</strong> Minimize a loss function by iteratively updating parameters in the direction of the steepest descent (negative gradient).</li>
<li><strong>Update Rule:</strong>
$$
\theta_{t+1} = \theta_t - \eta \cdot \nabla L(\theta_t)
$$
where:</li>
<li>$\theta_t$ are the parameters at iteration $t$</li>
<li>$\eta$ is the learning rate</li>
<li>$\nabla L(\theta_t)$ is the gradient of the loss function with respect to $\theta_t$</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li><strong>Fixed Learning Rate:</strong> Choosing an appropriate learning rate can be challenging; too high may cause divergence, too low may slow down convergence.</li>
<li><strong>No Adaptation:</strong> Doesn&rsquo;t adapt the learning rate based on the geometry of the loss surface, potentially leading to inefficient updates.</li>
</ul>
<h3 id="2-introducing-adam-optimizer"><strong>2. Introducing Adam Optimizer</strong><a hidden class="anchor" aria-hidden="true" href="#2-introducing-adam-optimizer">#</a></h3>
<p><strong>Adam (Adaptive Moment Estimation)</strong> combines the advantages of two other extensions of GD: <strong>Momentum</strong> and <strong>RMSProp</strong>. It computes adaptive learning rates for each parameter by maintaining estimates of both the first moment (mean) and the second moment (uncentered variance) of the gradients.</p>
<h4 id="key-concepts"><strong>Key Concepts:</strong><a hidden class="anchor" aria-hidden="true" href="#key-concepts">#</a></h4>
<ol>
<li><strong>Momentum:</strong></li>
</ol>
<ul>
<li>Helps accelerate GD in the relevant direction and dampens oscillations.</li>
<li>Maintains an exponentially decaying average of past gradients.</li>
</ul>
<ol start="2">
<li><strong>Adaptive Learning Rates:</strong></li>
</ol>
<ul>
<li>Adjusts the learning rate for each parameter individually based on the historical gradients.</li>
<li>Parameters with higher gradients receive smaller updates, and vice versa.</li>
</ul>
<p><strong>Adam</strong> effectively combines these by maintaining both moving averages and adapting learning rates accordingly.</p>
<h4 id="adams-update-mechanism"><strong>Adam&rsquo;s Update Mechanism:</strong><a hidden class="anchor" aria-hidden="true" href="#adams-update-mechanism">#</a></h4>
<p>Adam maintains two estimates for each parameter $\theta$:</p>
<ul>
<li><strong>First Moment ($m_t$):</strong> Estimate of the mean of the gradients.</li>
<li><strong>Second Moment ($v_t$):</strong> Estimate of the uncentered variance (mean of the squared gradients).</li>
</ul>
<p>The update steps are as follows:</p>
<ol>
<li><strong>Initialize Parameters:</strong></li>
</ol>
<ul>
<li>Initialize $m_0 = 0$ (first moment)</li>
<li>Initialize $v_0 = 0$ (second moment)</li>
<li>Choose hyperparameters:</li>
<li>Learning rate ($\eta$)</li>
<li>Decay rates for the moment estimates ($\beta_1$ for $m_t$, $\beta_2$ for $v_t$)</li>
<li>Small constant ($\epsilon$) to prevent division by zero</li>
</ul>
<ol start="2">
<li><strong>At each iteration $t$:</strong></li>
</ol>
<p>a. <strong>Compute Gradient:</strong>
$$
g_t = \nabla L(\theta_{t-1})
$$
where $L$ is the loss function.</p>
<p>b. <strong>Update First Moment ($m_t$):</strong>
$$
m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t
$$</p>
<ul>
<li>$m_t$ accumulates the gradients.</li>
</ul>
<p>c. <strong>Update Second Moment ($v_t$):</strong>
$$
v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2
$$</p>
<ul>
<li>$v_t$ accumulates the squared gradients.</li>
</ul>
<p>d. <strong>Bias Correction:</strong>
Since $m_t$ and $v_t$ are initialized at zero, they are biased towards zero, especially during the initial steps. To correct this bias:
$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
$$
$$
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$</p>
<ul>
<li>$\hat{m}_t$ and $\hat{v}_t$ are the bias-corrected estimates.</li>
</ul>
<p>e. <strong>Update Parameters:</strong>
$$
\theta_t = \theta_{t-1} - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$</p>
<ul>
<li>This step uses the adaptive learning rate for each parameter.</li>
</ul>
<h4 id="algorithm-summary"><strong>Algorithm Summary:</strong><a hidden class="anchor" aria-hidden="true" href="#algorithm-summary">#</a></h4>
<p>$$
\begin{align*}
&amp; \text{Initialize:} \quad m_0 = 0, \quad v_0 = 0, \quad t = 0 \[1em]
&amp; \text{For each iteration:} \[1em]
&amp; \quad t = t + 1 \[1em]
&amp; \quad g_t = \nabla L(\theta_{t-1}) \[1em]
&amp; \quad m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \[1em]
&amp; \quad v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2 \[1em]
&amp; \quad \hat{m}_t = \frac{m_t}{1 - \beta_1^t} \[1em]
&amp; \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t} \[1em]
&amp; \quad \theta_t = \theta_{t-1} - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align*}
$$</p>
<p>$$
\begin{align*}
&amp; \text{Initialize:} \quad m_0 = 0, \quad v_0 = 0, \quad t = 0 \
&amp; \text{For each iteration:} \
&amp; \quad t = t + 1 \
&amp; \quad g_t = \nabla L(\theta_{t-1}) \
&amp; \quad m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \
&amp; \quad v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2 \
&amp; \quad \hat{m}_t = \frac{m_t}{1 - \beta_1^t} \
&amp; \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t} \
&amp; \quad \theta_t = \theta_{t-1} - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align*}
$$</p>
<p>$$
\begin{align*}
&amp; \text{Initialize:} \quad m_0 = 0, \quad v_0 = 0, \quad t = 0 \
&amp; \text{For each iteration:} \
&amp; \quad t = t + 1 \
&amp; \quad g_t = \nabla L(\theta_{t-1}) \
&amp; \quad m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \
&amp; \quad v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2 \
&amp; \quad \hat{m}_t = \frac{m_t}{1 - \beta_1^t} \
&amp; \quad \hat{v}<em>t = \frac{v_t}{1 - \beta_2^t} \
&amp; \quad \theta_t = \theta</em>{t-1} - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align*}
$$</p>
<p>$$
\begin{align*}
&amp; \text{Initialize:} \quad m_0 = 0, \quad v_0 = 0, \quad t = 0 <br>
&amp; \text{For each iteration:} <br>
&amp; \quad t = t + 1 <br>
&amp; \quad g_t = \nabla L(\theta_{t-1}) <br>
&amp; \quad m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t <br>
&amp; \quad v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2 <br>
&amp; \quad \hat{m}_t = \frac{m_t}{1 - \beta_1^t} <br>
&amp; \quad \hat{v}<em>t = \frac{v_t}{1 - \beta_2^t} <br>
&amp; \quad \theta_t = \theta</em>{t-1} - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align*}
$$</p>
<p>$$
\begin{array}{ll}
\text{Initialize:} &amp; m_0 = 0, \quad v_0 = 0, \quad t = 0 <br>
\text{For each iteration:} &amp; <br>
&amp; t = t + 1 <br>
&amp; g_t = \nabla L(\theta_{t-1}) <br>
&amp; m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t <br>
&amp; v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2 <br>
&amp; \hat{m}_t = \frac{m_t}{1 - \beta_1^t} <br>
&amp; \hat{v}t = \frac{v_t}{1 - \beta_2^t} <br>
&amp; \theta_t = \theta{t-1} - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{array}
$$</p>
<p>$$
\begin{aligned}
&amp; \text{Initialize:} &amp;&amp; m_0 = 0, \quad v_0 = 0, \quad t = 0 \
&amp; \text{For each iteration:} &amp;&amp; \
&amp; &amp;&amp; t = t + 1 \
&amp; &amp;&amp; g_t = \nabla L(\theta_{t-1}) \
&amp; &amp;&amp; m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \
&amp; &amp;&amp; v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2 \
&amp; &amp;&amp; \hat{m}_t = \frac{m_t}{1 - \beta_1^t} \
&amp; &amp;&amp; \hat{v}<em>t = \frac{v_t}{1 - \beta_2^t} \
&amp; &amp;&amp; \theta_t = \theta</em>{t-1} - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{aligned}
$$</p>
<div class="equation">
  <p>Initialize: m<sub>0</sub> = 0, v<sub>0</sub> = 0, t = 0</p>
  <p>For each iteration:</p>
  <p style="margin-left: 20px;">t = t + 1</p>
  <p style="margin-left: 20px;">g<sub>t</sub> = ∇L(θ<sub>t-1</sub>)</p>
  <p style="margin-left: 20px;">m<sub>t</sub> = β<sub>1</sub> · m<sub>t-1</sub> + (1 - β<sub>1</sub>) · g<sub>t</sub></p>
  <p style="margin-left: 20px;">v<sub>t</sub> = β<sub>2</sub> · v<sub>t-1</sub> + (1 - β<sub>2</sub>) · g<sub>t</sub><sup>2</sup></p>
  <p style="margin-left: 20px;">m̂<sub>t</sub> = m<sub>t</sub> / (1 - β<sub>1</sub><sup>t</sup>)</p>
  <p style="margin-left: 20px;">v̂<sub>t</sub> = v<sub>t</sub> / (1 - β<sub>2</sub><sup>t</sup>)</p>
  <p style="margin-left: 20px;">θ<sub>t</sub> = θ<sub>t-1</sub> - η · m̂<sub>t</sub> / (√v̂<sub>t</sub> + ε)</p>
</div>
<p>comming back to the discussion of memory&hellip;</p>
<p>so now we know what to store cause of adam optimizer, the model weights, the gradients, and the optimizer states constitute the majority of the memory required for training.</p>
<p>but this is not the whole picture there is additional memory called termed Residual memory by the ZeRO pape,
this memory is made up of Activations, Temporary Buffers, and Memory Fragments.</p>
<p>Activations, are
The outputs of each sub-component (self-attention, feed-forward network) within each layer are activations.
The final output of each layer, which becomes the input to the next layer, is also an activation.</p>
<p>Large models with extensive architectures generate substantial activations. For example, training a 1.5B parameter GPT-2 model with a sequence length of 1,000 tokens and a batch size of 32 can consume around 60 GB of memory solely for activations.</p>
<p>To mitigate high memory usage, activation checkpointing (or recomputation) can be employed. This technique saves memory by storing only a subset of activations and recomputing the others as needed during the backward pass. While it reduces memory consumption (e.g., from 60 GB to 8 GB in the aforementioned example), it introduces a computational overhead of about 33% due to the additional recomputation.</p>
<p>so why do we need to store the activations, these are essential to for calucating the gradients in backprop,
<strong>temp buffers</strong>
Temporary buffers store intermediate results during operations like gradient all-reduce (used in distributed training) and gradient norm computation(used for gradient clipping). These buffers are essential for efficiently performing complex computations and communication between devices.
For large models, temporary buffers can require significant memory. For instance, a 1.5B parameter model might need around 6 GB of memory for a flattened fp32 buffer used during gradient all-reduce operations.</p>
<p><strong>Memory Fragmentation</strong>
Even after taking all of the above memory consumption into consideration, you migh still encounter the dreadful &ldquo;CUDA error: out of memory&rdquo;. This happens, because the gpu allocates memory in blcoks, even if ther is space for some of your params, but not all in a block of memory you will have issues.
In scenarios involving very large models, memory fragmentation can lead to out-of-memory (OOM) errors despite having a considerable amount of available memory. For example, training extremely large models might fail with over 30% of memory still free, but unusable due to fragmentation.</p>
<p>We cant do much about the Residual memory consumption, it is just good to have an idea about where did the memory go.</p>
<p>So lets take a look back at the memory we know we can control, ie the model weights, gradients and optimizer states.</p>
<p>We can use FP32 to store all of them but, its not necessary there is a concept of mixed precision training, which is widely adoppted in the industry, this inovlves stroing some of the components in lower precison FP16 (ie, half precision).</p>
<p>Lets take a look at this now.</p>
<div style="text-align: center;">
  <img src="/images/multi_gpu/mp_training.png" alt="TF32 Explained" style="display: block; margin: 0 auto;">
<p style="font-size: 0.8em; color: rgba(0, 0, 0, 0.6);">
  Figure 1: Comparison of FP8 and BF16 formats. Source: 
  <a href="https://arxiv.org/abs/xxxx.xxxxx" style="color: rgba(0, 0, 0, 0.6);">Smith et al. (2023)</a>
</p>
</div>
<p>The state-of-the-art approach to train large models on the current generation of NVIDIA GPUs is via mixed precision (fp16/32) training, where parameters and activations are stored as fp16, enabling the use of the high throughput tensor core units on these GPUs.</p>
<p>During mixed-precision training, both the forward and backward propagation are performed using fp16 weights and activations. However, to effectively compute and apply the updates at the end of the backward propagation, the mixed-precision optimizer keeps an fp32 copy of the parameters as well as an fp32 copy of all the other optimizer states.</p>
<p>Mixed precision training of a model with $\Psi$ parameters using Adam requires enough memory to hold an $\text{fp16}$ copy of the parameters and the gradients, with memory requirements of $2\Psi$ and $2\Psi$ bytes respectively. In addition, it needs to hold the optimizer states: an $\text{fp32}$ copy of the parameters, momentum, and variance, with memory requirements of $4\Psi$, $4\Psi$, and $4\Psi$ bytes, respectively. Let’s use $K$ to denote the memory multiplier of the optimizer states, i.e., the additional memory required to store them is $K\Psi$ bytes. Mixed-precision Adam has $K = 12$. In total, this results in $2\Psi + 2\Psi + K\Psi = 16\Psi$ bytes of memory requirement. For a model such as GPT-2 with 1.5 Billion parameters, this leads to a memory requirement of at least 24 GB, which is significantly higher than the meager 3 GB of memory required to hold the $\text{fp16}$ parameters alone.</p>
<p>so the above is true from a paper from ICLR 2018, after that people tried to train models in full FP16 format, which can be fruitful for smaller models, but below is an excerpt from the BLOOM model training ppl, who wrote a blogpost on huggingface.</p>
<blockquote>
<h3 id="bf16optimizer">BF16Optimizer<a hidden class="anchor" aria-hidden="true" href="#bf16optimizer">#</a></h3>
<p>Training huge LLM models in FP16 is a no-no.
We have proved it to ourselves by spending several months training a 104B model which as you can tell from the tensorboard was but a complete failure. We learned a lot of things while fighting the ever diverging lm-loss:</p>
 <div style="text-align: center;">
   <img src="/images/multi_gpu/tensorboard_hf_excerpt.png" alt="TF32 Explained" style="display: block; margin: 0 auto;">
 <p style="font-size: 0.8em; color: rgba(0, 0, 0, 0.6);">
   Figure 1: Comparison of FP8 and BF16 formats. Source: 
   <a href="https://arxiv.org/abs/xxxx.xxxxx" style="color: rgba(0, 0, 0, 0.6);">Smith et al. (2023)</a>
 </p>
 </div>
and we also got the same advice from the Megatron-LM and DeepSpeed teams after they trained the 530B model. The recent release of OPT-175B too reported that they had a very difficult time training in FP16.
</blockquote>
<p>So as you might rememember form the qunatization blogpost about, different datatypes, and my parise of the BF16 datatype.
The above is true if you have a V100 or older gpus that do not support the BF16 data type, but knowing the above is important because of historical reason or you might want to rent out a cheaper gpu fro training.</p>
<p>So how is Mixed precision training done using the BF16 fromat, lets take a look.</p>
<div style="text-align: center;">
  <img src="/images/multi_gpu/mp_bf16_training.png" alt="TF32 Explained" style="display: block; margin: 0 auto;">
<p style="font-size: 0.8em; color: rgba(0, 0, 0, 0.6);">
  Figure 1: Comparison of FP8 and BF16 formats. Source: 
  <a href="https://arxiv.org/abs/xxxx.xxxxx" style="color: rgba(0, 0, 0, 0.6);">Smith et al. (2023)</a>
</p>
</div>
<p>Here&rsquo;s a more concise explanation of the mixed precision data flow using BFLOAT16:</p>
<ol>
<li>
<p>Input: Activations from previous layer (L-1) in BFLOAT16.</p>
</li>
<li>
<p>Forward Pass:</p>
<ul>
<li>Multiply BFLOAT16 activations with BFLOAT16 weights.</li>
<li>Accumulate results in FP32.</li>
<li>Quantize output to BFLOAT16 for next layer&rsquo;s input.</li>
</ul>
</li>
<li>
<p>Backward Pass:
a) Error Gradients:</p>
<ul>
<li>Multiply BFLOAT16 error gradients with transposed weights.</li>
<li>Accumulate in FP32, then quantize to BFLOAT16.
b) Weight Gradients:</li>
<li>Multiply error gradients with transposed activations.</li>
<li>Accumulate results in FP32.</li>
</ul>
</li>
<li>
<p>Weight Update:</p>
<ul>
<li>Add FP32 weight gradients to FP32 master weights.</li>
</ul>
</li>
<li>
<p>Precision Management:</p>
<ul>
<li>Store master weights in FP32.</li>
<li>Use BFLOAT16 for computations and storage where possible.</li>
<li>Perform critical accumulations in FP32.</li>
<li>Convert between FP32 and BFLOAT16 as needed (shown as &lsquo;Q&rsquo; operations).</li>
</ul>
</li>
</ol>
<p>This approach balances computational efficiency (using BFLOAT16) with numerical stability (using FP32 for critical operations), enabling effective training of large neural networks.</p>
<p>So again this paper was published in 2019, and people now train the full model with the BF16 fromat since gpus like the A100 can support these kinds of formats.</p>
<p>Lets take a look at at an even smaller format FP8, now people do train llms on this format. (this is a more latest paper came out in 2022).
this paper uses fp8 for the paramaters and gradients, but generally the optimizer states need to be stored in BF16 or higher.</p>
<p>So now we some knowledge about the adam optimizer, its weight componets, and finally mixed precision training.</p>
<p>Lest start Multi GPU Training</p>
<p>start with DDP or distributed data parallel.</p>
<h2 id="ddp-distributed-data-parallel">DDP (Distributed Data Parallel)<a hidden class="anchor" aria-hidden="true" href="#ddp-distributed-data-parallel">#</a></h2>
<p>basically ddp is quite easy, most of the effort of ddp lies in making efficient in actula production, dealing wiht race conditions etc&hellip;</p>
<h2 id="pipeline-model-parallelism">Pipeline model parallelism<a hidden class="anchor" aria-hidden="true" href="#pipeline-model-parallelism">#</a></h2>
<p>In data parallelism we say how we can fit multiple copies of the same model on different GPUs, but now we consider the more common scenarion of the model not being able to fit on a single gpu,
There are primarily two ways we can tackle this problem pipeline parallelism is the more intuitive one, so lets start with that.</p>
<p>The basic idea of pipeline parallelism is quite simple, if your model dosent fit on a single GPU, slice up the different layers and put them across multiple gpus, so each gpu takes input as the output of the previous partition as input, but the problem here is obvious, you cant rent a h100 gpu cluster for 8 bucks an hour and have this bad gpu utilization, so here are some techniques that make model parallelism efficient</p>
<ol>
<li><strong>PipeDream: Generalized Pipeline Parallelism for DNN Training</strong></li>
</ol>
<p><strong>Pipeline Parallelism in Deep Learning Training: An In-Depth Explanation Inspired by the PipeDream Paper</strong></p>
<hr>
<p>Pipeline parallelism is a technique used to accelerate the training of deep neural networks (DNNs) by partitioning the computation graph across multiple devices, such as GPUs. The PipeDream paper introduces a novel approach to pipeline parallelism that addresses the limitations of traditional data and model parallelism methods. Below is a detailed explanation of pipeline parallelism as described in the PipeDream paper.</p>
<h3 id="background"><strong>Background</strong><a hidden class="anchor" aria-hidden="true" href="#background">#</a></h3>
<p>Traditional parallelization strategies for training DNNs include:</p>
<ol>
<li>
<p><strong>Data Parallelism (DP):</strong> Distributes different data samples (minibatches) across multiple GPUs, each with a complete copy of the model. After computing gradients, the GPUs synchronize to update the model parameters.</p>
</li>
<li>
<p><strong>Model Parallelism (MP):</strong> Splits the model itself across multiple GPUs. Each GPU holds a portion of the model and processes the same data sample sequentially through the different parts.</p>
</li>
</ol>
<p>While these methods have their advantages, they also have limitations, especially when scaling to large models or a high number of GPUs. Pipeline parallelism aims to overcome these limitations by combining aspects of both data and model parallelism.</p>
<h3 id="what-is-pipeline-parallelism"><strong>What is Pipeline Parallelism?</strong><a hidden class="anchor" aria-hidden="true" href="#what-is-pipeline-parallelism">#</a></h3>
<p>Pipeline parallelism involves dividing the layers of a DNN into sequential stages and assigning each stage to a different GPU. Each GPU is responsible for the forward and backward computations of its assigned layers. By injecting multiple minibatches into the pipeline, all GPUs can work simultaneously, processing different minibatches at different stages.</p>
<h3 id="how-pipeline-parallelism-works"><strong>How Pipeline Parallelism Works</strong><a hidden class="anchor" aria-hidden="true" href="#how-pipeline-parallelism-works">#</a></h3>
<ol>
<li>
<p><strong>Partitioning the Model:</strong></p>
<ul>
<li>The DNN is divided into several stages, each containing a consecutive set of layers.</li>
<li>Each stage is assigned to a separate GPU.</li>
</ul>
</li>
<li>
<p><strong>Injecting Minibatches:</strong></p>
<ul>
<li>Multiple minibatches are introduced into the pipeline sequentially.</li>
<li>As one GPU completes the forward pass for a minibatch, it sends the output activations to the next GPU and starts processing the next minibatch.</li>
</ul>
</li>
<li>
<p><strong>Forward and Backward Passes:</strong></p>
<ul>
<li>The last stage (GPU) starts the backward pass immediately after completing the forward pass for a minibatch.</li>
<li>Each GPU performs the backward pass for its stage and sends the gradients to the previous GPU while starting computations for the next minibatch.</li>
</ul>
</li>
<li>
<p><strong>Asynchronous Communication:</strong></p>
<ul>
<li>Communication of activations and gradients between GPUs is done asynchronously.</li>
<li>This allows for overlapping computation and communication, improving overall efficiency.</li>
</ul>
</li>
</ol>
<h3 id="advantages-of-pipeline-parallelism"><strong>Advantages of Pipeline Parallelism</strong><a hidden class="anchor" aria-hidden="true" href="#advantages-of-pipeline-parallelism">#</a></h3>
<ol>
<li>
<p><strong>Reduced Communication Overhead:</strong></p>
<ul>
<li>Communication is limited to adjacent GPUs, transferring only the necessary activations and gradients.</li>
<li>This is more efficient than DP, which requires global synchronization and communication of all model parameters.</li>
</ul>
</li>
<li>
<p><strong>Improved Resource Utilization:</strong></p>
<ul>
<li>By keeping multiple minibatches in flight, all GPUs remain active, reducing idle time.</li>
<li>Overlapping computation and communication maximizes hardware utilization.</li>
</ul>
</li>
</ol>
<h3 id="challenges-and-solutions-in-pipedream"><strong>Challenges and Solutions in PipeDream</strong><a hidden class="anchor" aria-hidden="true" href="#challenges-and-solutions-in-pipedream">#</a></h3>
<p>The PipeDream paper identifies three main challenges in implementing effective pipeline parallelism and proposes solutions for each.</p>
<h4 id="challenge-1-work-partitioning"><strong>Challenge 1: Work Partitioning</strong><a hidden class="anchor" aria-hidden="true" href="#challenge-1-work-partitioning">#</a></h4>
<p><strong>Problem:</strong></p>
<ul>
<li>Uneven computational workloads across stages can lead to pipeline bubbles, where some GPUs are idle waiting for others to complete.</li>
<li>Excessive communication between GPUs can reduce throughput.</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li><strong>Automated Partitioning Algorithm:</strong>
<ul>
<li>Profiles the DNN to estimate computation times and output sizes for each layer.</li>
<li>Uses dynamic programming to partition layers into stages such that each stage has a balanced computational load.</li>
<li>Takes into account hardware topology and communication bandwidth to minimize communication overhead.</li>
<li>Allows for stage replication (using data parallelism within a stage) when perfect load balancing isn&rsquo;t possible with simple partitioning.</li>
</ul>
</li>
</ul>
<p><strong>Process:</strong></p>
<ol>
<li><strong>Profiling:</strong>
<ul>
<li>Measure computation times (forward and backward passes) and activation sizes for each layer.</li>
</ul>
</li>
<li><strong>Optimization:</strong>
<ul>
<li>Solve a dynamic programming problem to find the optimal partitioning that balances the workload and minimizes communication.</li>
<li>Consider replication factors for stages to further balance the pipeline.</li>
</ul>
</li>
</ol>
<h4 id="challenge-2-work-scheduling"><strong>Challenge 2: Work Scheduling</strong><a hidden class="anchor" aria-hidden="true" href="#challenge-2-work-scheduling">#</a></h4>
<p><strong>Problem:</strong></p>
<ul>
<li>Deciding whether a GPU should perform a forward or backward pass at any given time.</li>
<li>Routing minibatches correctly when stages are replicated.</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li>
<p><strong>One-Forward-One-Backward (1F1B) Scheduling:</strong></p>
<ul>
<li>Each GPU alternates between performing a forward pass for one minibatch and a backward pass for another minibatch.</li>
<li>This schedule ensures that all GPUs are continuously utilized.</li>
</ul>
</li>
<li>
<p><strong>Deterministic Round-Robin Load Balancing (1F1B-RR):</strong></p>
<ul>
<li>When stages are replicated, minibatches are assigned to replicas in a round-robin fashion based on their IDs.</li>
<li>Ensures that each minibatch is processed by the same GPU for both forward and backward passes within a stage.</li>
</ul>
</li>
</ul>
<p><strong>Process:</strong></p>
<ol>
<li><strong>Startup Phase:</strong>
<ul>
<li>The pipeline is filled with an optimal number of minibatches to reach steady state.</li>
</ul>
</li>
<li><strong>Steady State:</strong>
<ul>
<li>GPUs follow the 1F1B schedule, maintaining a balance between forward and backward computations.</li>
</ul>
</li>
</ol>
<h4 id="challenge-3-effective-learning"><strong>Challenge 3: Effective Learning</strong><a hidden class="anchor" aria-hidden="true" href="#challenge-3-effective-learning">#</a></h4>
<p><strong>Problem:</strong></p>
<ul>
<li>Inconsistency in parameter versions used during forward and backward passes can lead to invalid gradients and hinder convergence.</li>
<li>Since parameters are updated asynchronously across stages, a minibatch might use different parameter versions in its forward and backward passes.</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li>
<p><strong>Weight Stashing:</strong></p>
<ul>
<li>Store (stash) the parameters used during the forward pass of each minibatch.</li>
<li>Use the same stashed parameters during the backward pass to compute gradients.</li>
<li>Ensures that gradients are computed consistently with the parameters used in the forward pass.</li>
</ul>
</li>
<li>
<p><strong>Vertical Sync (Optional):</strong></p>
<ul>
<li>Coordinates the use of parameter versions across stages.</li>
<li>Each minibatch uses the same parameter version for both forward and backward passes across all stages.</li>
<li>Involves more coordination and storage but provides consistency similar to synchronous data parallelism.</li>
</ul>
</li>
</ul>
<p><strong>Process:</strong></p>
<ol>
<li><strong>During Forward Pass:</strong>
<ul>
<li>Use the latest available parameters.</li>
<li>Stash the parameters for each minibatch.</li>
</ul>
</li>
<li><strong>During Backward Pass:</strong>
<ul>
<li>Retrieve the stashed parameters corresponding to the minibatch.</li>
<li>Compute gradients and update parameters accordingly.</li>
</ul>
</li>
</ol>
<h3 id="understanding-staleness-and-consistency"><strong>Understanding Staleness and Consistency</strong><a hidden class="anchor" aria-hidden="true" href="#understanding-staleness-and-consistency">#</a></h3>
<ul>
<li>
<p><strong>Staleness:</strong></p>
<ul>
<li>Refers to the difference in parameter versions used when computing gradients.</li>
<li>Weight stashing reduces staleness within a stage but doesn&rsquo;t eliminate it across stages.</li>
</ul>
</li>
<li>
<p><strong>Consistency Models:</strong></p>
<ul>
<li><strong>Without Weight Stashing:</strong> Parameters may be inconsistent, leading to invalid gradients.</li>
<li><strong>With Weight Stashing:</strong> Consistent within a stage; some staleness across stages.</li>
<li><strong>With Vertical Sync:</strong> Consistent across all stages for each minibatch; mimics synchronous training.</li>
</ul>
</li>
</ul>
<h3 id="memory-considerations"><strong>Memory Considerations</strong><a hidden class="anchor" aria-hidden="true" href="#memory-considerations">#</a></h3>
<ul>
<li>
<p><strong>Memory Overhead:</strong></p>
<ul>
<li>Weight stashing increases memory usage since parameters need to be stored for each in-flight minibatch.</li>
<li>However, the per-GPU memory usage remains comparable to data parallelism.</li>
</ul>
</li>
<li>
<p><strong>Optimization Techniques:</strong></p>
<ul>
<li><strong>Activation Recomputation:</strong> Discard activations after forward pass and recompute them during backward pass to save memory.</li>
<li><strong>Gradient Accumulation:</strong> Aggregate gradients over multiple minibatches before updating parameters.</li>
</ul>
</li>
</ul>
<h3 id="implementation-highlights"><strong>Implementation Highlights</strong><a hidden class="anchor" aria-hidden="true" href="#implementation-highlights">#</a></h3>
<ul>
<li>
<p><strong>PipeDream Runtime:</strong></p>
<ul>
<li>Manages device memory, schedules tasks, and handles communication between GPUs.</li>
<li>Integrates with deep learning frameworks like PyTorch.</li>
</ul>
</li>
<li>
<p><strong>Communication Backend:</strong></p>
<ul>
<li>Uses efficient communication libraries (e.g., Gloo, NCCL) for transferring activations and gradients.</li>
</ul>
</li>
<li>
<p><strong>Checkpointing:</strong></p>
<ul>
<li>Supports periodic saving of model parameters for fault tolerance.</li>
<li>Each stage checkpoints independently, reducing coordination overhead.</li>
</ul>
</li>
</ul>
<h3 id="benefits-of-pipedreams-pipeline-parallelism"><strong>Benefits of PipeDream&rsquo;s Pipeline Parallelism</strong><a hidden class="anchor" aria-hidden="true" href="#benefits-of-pipedreams-pipeline-parallelism">#</a></h3>
<ul>
<li>
<p><strong>Scalability:</strong></p>
<ul>
<li>Enables training of larger models that don&rsquo;t fit into the memory of a single GPU.</li>
<li>Efficiently utilizes multiple GPUs without incurring excessive communication overhead.</li>
</ul>
</li>
<li>
<p><strong>Throughput Improvement:</strong></p>
<ul>
<li>By keeping all GPUs busy and overlapping computation with communication, PipeDream achieves higher throughput compared to traditional methods.</li>
</ul>
</li>
<li>
<p><strong>Flexibility:</strong></p>
<ul>
<li>Can be combined with data parallelism within stages (hybrid parallelism) for further scalability.</li>
</ul>
</li>
</ul>
<h3 id="conclusion"><strong>Conclusion</strong><a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>Pipeline parallelism, as implemented in the PipeDream paper, presents an effective method for scaling DNN training across multiple GPUs. By carefully partitioning the model, scheduling work to maximize GPU utilization, and ensuring consistent parameter usage through weight stashing, PipeDream overcomes the challenges associated with pipeline parallelism. This approach leads to significant improvements in training throughput while maintaining model convergence, making it a valuable technique for training large-scale deep learning models.</p>
<p>8:37
parallelism uh we split the layers or operators in the model over multiple devices uh and then we also will split
8:45
each batch of inputs into smaller micro batches and then paralyze execution across these micro
8:52
batches to be very concrete uh let&rsquo;s look at this visually um so this is a
8:57
model uh that we&rsquo;re splitting over four devices uh so let&rsquo;s say that if the if the model
9:06
has eight Transformer layers uh what we&rsquo;re going to do is we&rsquo;re going to assign the first two Transformer layers
9:12
to the first device the next two to the second device and so on now in order to perform a single
9:18
forward and backward path through the model we&rsquo;re going to need to take a single input pass it through device one
9:26
device one performs its computation uh represented by this blue box computes an
9:32
what what we call an output activation and then this output activation needs to be communicated to the next device uh uh
9:40
and and and and and the second device can start it uh it its computation until it&rsquo;s receive this activation from the
9:47
the first device um and and so what that means is that there is this sequential
9:53
data dependency across each of these devices um and lots of these devices are
9:58
idle um in in particular at any point in time only one device is
10:04
active um and so so very quickly you can see that uh this scheme has uh pretty uh
10:11
poor utilization and low throughput so instead what we can do is we can take this input batch a um and
10:19
split it into smaller micro batches uh let&rsquo;s say that this this uh input batch
10:26
a has has four inputs in it um what we can do is we can split that um input of
10:32
uh input batch of four into four micro batches of size one and then pipeline execution across um those micro
10:41
batches um in particular um this is this is what this looks like um we note now
10:46
that we only have sequential uh sequential data dependencies um between
10:53
uh devices for a given microbatch um in other words um device 2 now only needs
10:59
to wait on device one for um uh this output activation of microbatch A1
11:06
before it starts computation so no longer do you have to wait for all four for for device one to complete uh
11:12
computation for all four um input samples in in in in in this patch um
11:18
instead we can just um we can immediately start uh processing on device 2 as soon as just an a single
11:26
input&rsquo;s uh worth of computation is is completed on on device
11:35
one after we complete uh uh computation for all of these forward and and
11:42
backward passes for these four um uh micro batches uh then we can step the
11:47
the optimizer uh which is basically around here um and then we can update
11:54
the the weights and move on to the next training iteration
12:00
it&rsquo;s easy to see from from from these figures that uh this is much more efficient um compared to the the naive
12:07
case where um we only have a single batch uh but there are still some idle
12:14
periods we haven&rsquo;t completely eliminated um these idle periods from
12:20
from from these timelines um we call um the periods of of time that each device
12:27
is is Idle um uh at the start and end of a training iteration the pipeline flush
12:33
um and and these are basically fundamental right um uh basically the pipeline flush is the time that devices
12:39
need to wait for inputs to actually flow through the the pipeline um and then
12:45
subsequently get drained
12:52
out so to summarize with pipeline model parallelism we need to perform uh point-to-point communication between
12:58
between consecutive pipeline stages uh and we have these pipeline Bubbles at the start and end of every
13:05
batch we can actually exactly quantify how much time is spent in the pipeline bubble uh it&rsquo;s actually going to be
13:11
equal to P minus one microb batches worth of forward and backward passes uh
13:16
where p is the number of pipeline stages um so in the previous figure uh the uh
13:23
the number of pipeline stages was four um and the size of the pipeline bubble was three micro batches worth of forward
13:31
and and backward pass</p>
<hr>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] <a href="https://huggingface.co/blog/bloom-megatron-deepspeed#bf16optimizer">https://huggingface.co/blog/bloom-megatron-deepspeed#bf16optimizer</a></p>
<p>[2] <a href="https://lightning.ai/blog/doubling-neural-network-finetuning-efficiency-with-16-bit-precision-techniques/">https://lightning.ai/blog/doubling-neural-network-finetuning-efficiency-with-16-bit-precision-techniques/</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer><script src="https://utteranc.es/client.js"
        repo="Shahid-Mo/Shahid-Mo.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://localhost:1313/">TensorTunes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
