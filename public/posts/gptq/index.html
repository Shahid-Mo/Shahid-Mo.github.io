<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=34579&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Quantization in LLMS Part 2: GPTQ | TensorTunes</title>
<meta name="keywords" content="">
<meta name="description" content="Quantization Method On the fly quantization CPU CUDA GPU RoCm GPU (AMD) Metal (Apple Silicon) torch.compile() support Number of bits Supports fine-tuning (through PEFT) Serializable with ü§ó transformers ü§ó transformers support AQLM ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè 1 / 2 ‚óè ‚óè ‚óè AWQ ‚óè ‚óè ‚óè ‚óè ‚óè ? 4 ‚óè ‚óè ‚óè bitsandbytes ‚óè ‚óè ‚óè ‚óè ‚óè 4 / 8 ‚óè ‚óè ‚óè EETQ ‚óè ‚óè ‚óè ‚óè ?">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:34579/posts/gptq/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:34579/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:34579/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:34579/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:34579/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:34579/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:34579/posts/gptq/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:34579/" accesskey="h" title="TensorTunes (Alt + H)">TensorTunes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:34579/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:34579/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Quantization in LLMS Part 2: GPTQ
    </h1>
    <div class="post-meta"><span title='2024-08-01 09:51:17 -0400 EDT'>August 1, 2024</span>

</div>
  </header> 
  <div class="post-content"><table>
<thead>
<tr>
<th>Quantization Method</th>
<th>On the fly quantization</th>
<th>CPU</th>
<th>CUDA GPU</th>
<th>RoCm GPU (AMD)</th>
<th>Metal (Apple Silicon)</th>
<th>torch.compile() support</th>
<th>Number of bits</th>
<th>Supports fine-tuning (through PEFT)</th>
<th>Serializable with ü§ó transformers</th>
<th>ü§ó transformers support</th>
</tr>
</thead>
<tbody>
<tr>
<td>AQLM</td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td>1 / 2</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
</tr>
<tr>
<td>AWQ</td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td>?</td>
<td>4</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
</tr>
<tr>
<td>bitsandbytes</td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td>4 / 8</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td></td>
</tr>
<tr>
<td>EETQ</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td>?</td>
<td>8</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td></td>
</tr>
<tr>
<td>GGUF / GGML</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td>1 - 8</td>
<td>See GGUF section</td>
<td>See GGUF section</td>
<td>See GGUF section</td>
<td></td>
</tr>
<tr>
<td>GPTQ</td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td>2 - 3 - 4 - 8</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td></td>
</tr>
<tr>
<td>HQQ</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td>1 - 8</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td></td>
</tr>
<tr>
<td>Quanto</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td>2 / 4 / 8</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td></td>
</tr>
<tr>
<td>FBGEMM_FP8</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td>8</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td></td>
</tr>
<tr>
<td>torchao</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td><span style="color:red">‚óè</span></td>
<td>partial support (int4 weight only)</td>
<td>4 / 8</td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td><span style="color:green">‚óè</span></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Introduction</strong></p>
<p>Quantization is a crucial technique in deep learning that reduces the memory footprint and computational requirements of neural networks by representing weights and activations with lower-precision numerical formats. This is particularly important when deploying large models on devices with limited resources. However, quantizing a neural network without significantly degrading its performance is challenging.</p>
<p>The GPTQ (Gradient Post-Training Quantization) algorithm is a method designed to efficiently quantize large-scale neural networks, such as those used in natural language processing, while maintaining high accuracy. GPTQ builds upon previous methods like Optimal Brain Quantization (OBQ) but introduces significant modifications to make it scalable to models with billions of parameters.</p>
<p>In this explanation, we will delve into the mathematical foundations of GPTQ, explain how it leverages the Hessian matrix and its inverse, discuss the role of the Cholesky decomposition, and provide a detailed walkthrough of the algorithm. We will also include examples to illustrate key concepts.</p>
<hr>
<h3 id="1-problem-statement"><strong>1. Problem Statement</strong><a hidden class="anchor" aria-hidden="true" href="#1-problem-statement">#</a></h3>
<p>Given a pre-trained neural network, our goal is to quantize its weights so that the network&rsquo;s output remains as close as possible to that of the original network when processing a set of inputs. Specifically, for a linear (fully connected) layer, we aim to find a quantized weight matrix $ \mathbf{W}_c $ that minimizes the reconstruction error.</p>
<p><strong>Mathematically</strong>, the objective is:</p>
<p>$$
\underset{\mathbf{W}_c}{\text{argmin}} , | \mathbf{W}\mathbf{X} - \mathbf{W}_c \mathbf{X} |_2^2
$$</p>
<ul>
<li>$ \mathbf{W} $: Original full-precision weight matrix of the layer.</li>
<li>$ \mathbf{W}_c $: Quantized weight matrix we want to find.</li>
<li>$ \mathbf{X} $: Input matrix to the layer (a set of $ m $ input examples).</li>
<li>$ | \cdot |_2 $: Frobenius norm, summing over all elements.</li>
</ul>
<p><strong>Goal</strong>: Find $ \mathbf{W}_c $ that minimizes the output difference caused by quantization.</p>
<hr>
<h3 id="2-optimal-brain-quantization-obq"><strong>2. Optimal Brain Quantization (OBQ)</strong><a hidden class="anchor" aria-hidden="true" href="#2-optimal-brain-quantization-obq">#</a></h3>
<h4 id="21-row-wise-independent-quantization"><strong>2.1. Row-wise Independent Quantization</strong><a hidden class="anchor" aria-hidden="true" href="#21-row-wise-independent-quantization">#</a></h4>
<p>OBQ simplifies the problem by treating each row $ \mathbf{w} $ of $ \mathbf{W} $ independently. This is reasonable because in a fully connected layer, each output neuron corresponds to one row of $ \mathbf{W} $, and the neurons operate independently given the inputs.</p>
<p><strong>Objective per row</strong>:</p>
<p>$$
\underset{\mathbf{w}_c}{\text{argmin}} , | \mathbf{w}\mathbf{X} - \mathbf{w}_c \mathbf{X} |_2^2
$$</p>
<h4 id="22-quadratic-formulation"><strong>2.2. Quadratic Formulation</strong><a hidden class="anchor" aria-hidden="true" href="#22-quadratic-formulation">#</a></h4>
<p>The error for each row can be expressed as:</p>
<p>$$
E(\mathbf{w}_c) = | \mathbf{w}\mathbf{X} - \mathbf{w}_c \mathbf{X} |_2^2 = (\mathbf{w} - \mathbf{w}_c) \mathbf{H} (\mathbf{w} - \mathbf{w}_c)^\top
$$</p>
<p>Where:</p>
<ul>
<li>$ \mathbf{H} = \mathbf{X}\mathbf{X}^\top $: The Hessian matrix for this quadratic form.</li>
<li>$ \mathbf{w} $: Original weights (vector).</li>
<li>$ \mathbf{w}_c $: Quantized weights (vector).</li>
</ul>
<p>The Hessian $ \mathbf{H} $ captures the second-order derivatives of the error with respect to $ \mathbf{w}_c $.</p>
<h4 id="23-greedy-quantization"><strong>2.3. Greedy Quantization</strong><a hidden class="anchor" aria-hidden="true" href="#23-greedy-quantization">#</a></h4>
<p>OBQ quantizes one weight at a time:</p>
<ol>
<li>
<p><strong>Select weight to quantize</strong>: Choose the weight that, when quantized, results in the smallest increase in the error $ E $.</p>
</li>
<li>
<p><strong>Update remaining weights</strong>: Adjust the unquantized weights to compensate for the error introduced by quantizing the selected weight.</p>
</li>
</ol>
<h4 id="24-mathematical-derivation"><strong>2.4. Mathematical Derivation</strong><a hidden class="anchor" aria-hidden="true" href="#24-mathematical-derivation">#</a></h4>
<p><strong>Step 1: Quantization Error for a Single Weight</strong></p>
<p>Let‚Äôs consider quantizing the $ q $-th weight $ w_q $ in $ \mathbf{w} $:</p>
<p>$$
\delta w_q = w_q - \text{quant}(w_q)
$$</p>
<p>The change in the error due to quantizing $ w_q $ is:</p>
<p>$$
\Delta E = (\delta w_q)^2 H_{qq}
$$</p>
<p>Where $ H_{qq} $ is the $ q $-th diagonal element of $ \mathbf{H} $.</p>
<p><strong>Step 2: Updating Remaining Weights</strong></p>
<p>To minimize the error, we adjust the remaining unquantized weights $ \mathbf{w}_F $:</p>
<p>$$
\delta \mathbf{w}<em>F = -\frac{\delta w_q}{H</em>{qq}} \mathbf{H}_{Fq}
$$</p>
<ul>
<li>$ \mathbf{H}_{Fq} $: The $ q $-th column (excluding $ q $-th row) of $ \mathbf{H} $.</li>
</ul>
<p>This adjustment aims to compensate for the error introduced by quantizing $ w_q $.</p>
<p><strong>Step 3: Update Hessian Inverse</strong></p>
<p>After quantizing $ w_q $, we need to update the inverse Hessian $ \mathbf{H}_F^{-1} $ for the remaining weights:</p>
<p>$$
\mathbf{H}_F^{-1} \leftarrow \mathbf{H}_F^{-1} - \frac{\mathbf{H}_F^{-1} \mathbf{e}_q \mathbf{e}_q^\top \mathbf{H}<em>F^{-1}}{H</em>{qq}}
$$</p>
<ul>
<li>$ \mathbf{e}_q $: Standard basis vector with 1 at position $ q $ and zeros elsewhere.</li>
</ul>
<p><strong>Note</strong>: This update uses the Sherman-Morrison formula for rank-one updates of matrix inverses.</p>
<hr>
<h3 id="3-limitations-of-obq"><strong>3. Limitations of OBQ</strong><a hidden class="anchor" aria-hidden="true" href="#3-limitations-of-obq">#</a></h3>
<p>While OBQ is effective for small to medium-sized models, it faces challenges with large models:</p>
<ul>
<li>
<p><strong>Computational Complexity</strong>: The algorithm has cubic time complexity $ O(d_{\text{row}} \cdot d_{\text{col}}^3) $, where $ d_{\text{row}} $ and $ d_{\text{col}} $ are the dimensions of $ \mathbf{W} $.</p>
</li>
<li>
<p><strong>Memory Requirements</strong>: Storing and updating the Hessian inverse becomes impractical for layers with millions of parameters.</p>
</li>
</ul>
<hr>
<h3 id="4-gptq-algorithm"><strong>4. GPTQ Algorithm</strong><a hidden class="anchor" aria-hidden="true" href="#4-gptq-algorithm">#</a></h3>
<p>GPTQ introduces several key modifications to make the quantization process scalable:</p>
<h4 id="41-arbitrary-quantization-order"><strong>4.1. Arbitrary Quantization Order</strong><a hidden class="anchor" aria-hidden="true" href="#41-arbitrary-quantization-order">#</a></h4>
<p><strong>Insight</strong>: Quantizing weights in an arbitrary fixed order (e.g., left to right) performs almost as well as the greedy order, especially for large layers.</p>
<p><strong>Benefit</strong>: This allows all rows to quantize weights in the same order, making $ \mathbf{H} $ and its inverse the same across rows.</p>
<h4 id="42-shared-hessian-inverse"><strong>4.2. Shared Hessian Inverse</strong><a hidden class="anchor" aria-hidden="true" href="#42-shared-hessian-inverse">#</a></h4>
<p>Since all rows share the same quantization order, we can compute $ \mathbf{H}^{-1} $ once and use it for all rows.</p>
<ul>
<li><strong>Computational Saving</strong>: Reduces complexity from $ O(d_{\text{row}} \cdot d_{\text{col}}^3) $ to $ O(\max{d_{\text{row}} \cdot d_{\text{col}}^2, d_{\text{col}}^3}) $.</li>
</ul>
<h4 id="43-lazy-batch-updates"><strong>4.3. Lazy Batch Updates</strong><a hidden class="anchor" aria-hidden="true" href="#43-lazy-batch-updates">#</a></h4>
<p>To improve computational efficiency:</p>
<ul>
<li>
<p><strong>Process Blocks</strong>: Quantize weights in blocks of columns (e.g., 128 columns at a time).</p>
</li>
<li>
<p><strong>Batch Updates</strong>: Update $ \mathbf{W} $ and $ \mathbf{H}^{-1} $ after processing each block, rather than after every weight quantization.</p>
</li>
</ul>
<p><strong>Benefit</strong>: Increases the compute-to-memory-access ratio, better utilizing GPU capabilities.</p>
<h4 id="44-cholesky-decomposition"><strong>4.4. Cholesky Decomposition</strong><a hidden class="anchor" aria-hidden="true" href="#44-cholesky-decomposition">#</a></h4>
<p>To address numerical instability:</p>
<ul>
<li>
<p><strong>Observation</strong>: Only certain parts (rows/columns) of $ \mathbf{H}^{-1} $ are needed during quantization.</p>
</li>
<li>
<p><strong>Solution</strong>: Use the Cholesky decomposition of $ \mathbf{H}^{-1} $ to compute necessary components more stably.</p>
</li>
<li>
<p><strong>Cholesky Decomposition</strong>: For a symmetric positive-definite matrix $ \mathbf{A} $, the Cholesky decomposition finds a lower triangular matrix $ \mathbf{L} $ such that $ \mathbf{A} = \mathbf{L} \mathbf{L}^\top $.</p>
</li>
</ul>
<p><strong>Benefit</strong>: Enhances numerical stability and reduces computational errors, especially important for large models.</p>
<hr>
<h3 id="5-mathematical-details"><strong>5. Mathematical Details</strong><a hidden class="anchor" aria-hidden="true" href="#5-mathematical-details">#</a></h3>
<h4 id="51-hessian-matrix--mathbfh-"><strong>5.1. Hessian Matrix $ \mathbf{H} $</strong><a hidden class="anchor" aria-hidden="true" href="#51-hessian-matrix--mathbfh-">#</a></h4>
<p><strong>Definition</strong>:</p>
<p>$$
\mathbf{H} = 2 \mathbf{X} \mathbf{X}^\top + \lambda \mathbf{I}
$$</p>
<ul>
<li>$ \mathbf{X} \in \mathbb{R}^{d_{\text{col}} \times m} $: Input matrix with $ m $ examples.</li>
<li>$ \lambda $: Damping factor added to ensure numerical stability (e.g., $ \lambda = 0.01 \times \text{mean of diagonal elements} $).</li>
<li>$ \mathbf{I} $: Identity matrix.</li>
</ul>
<p><strong>Role</strong>: Captures the curvature of the error function with respect to the weights.</p>
<p><strong>Note</strong>: The factor of 2 arises from the derivative of the squared error.</p>
<h4 id="52-inverse-hessian--mathbfh-1-"><strong>5.2. Inverse Hessian $ \mathbf{H}^{-1} $</strong><a hidden class="anchor" aria-hidden="true" href="#52-inverse-hessian--mathbfh-1-">#</a></h4>
<ul>
<li>
<p><strong>Purpose</strong>: Required to compute the optimal adjustments to the unquantized weights when quantizing a weight.</p>
</li>
<li>
<p><strong>Computation</strong>: Direct inversion is computationally expensive for large matrices.</p>
</li>
</ul>
<h4 id="53-cholesky-decomposition"><strong>5.3. Cholesky Decomposition</strong><a hidden class="anchor" aria-hidden="true" href="#53-cholesky-decomposition">#</a></h4>
<p><strong>Concept</strong>:</p>
<ul>
<li>For a symmetric positive-definite matrix $ \mathbf{A} $, the Cholesky decomposition finds a lower triangular matrix $ \mathbf{L} $ such that:</li>
</ul>
<p>$$
\mathbf{A} = \mathbf{L} \mathbf{L}^\top
$$</p>
<p><strong>Significance</strong>:</p>
<ul>
<li>
<p><strong>Numerical Stability</strong>: More stable than direct inversion or eigenvalue decomposition for positive-definite matrices.</p>
</li>
<li>
<p><strong>Efficient Computation</strong>: Allows solving linear systems $ \mathbf{A}\mathbf{x} = \mathbf{b} $ efficiently by forward and backward substitution.</p>
</li>
</ul>
<p><strong>Application in GPTQ</strong>:</p>
<ul>
<li>
<p>Instead of computing $ \mathbf{H}^{-1} $ directly, compute the Cholesky decomposition $ \mathbf{L} $.</p>
</li>
<li>
<p>Use $ \mathbf{L} $ to compute necessary components of $ \mathbf{H}^{-1} $ when needed.</p>
</li>
</ul>
<hr>
<h3 id="6-gptq-algorithm-steps"><strong>6. GPTQ Algorithm Steps</strong><a hidden class="anchor" aria-hidden="true" href="#6-gptq-algorithm-steps">#</a></h3>
<h4 id="61-initialization"><strong>6.1. Initialization</strong><a hidden class="anchor" aria-hidden="true" href="#61-initialization">#</a></h4>
<ul>
<li><strong>Compute Hessian Matrix</strong>:</li>
</ul>
<p>$$
\mathbf{H} = 2 \mathbf{X} \mathbf{X}^\top + \lambda \mathbf{I}
$$</p>
<ul>
<li><strong>Compute Cholesky Decomposition</strong>:</li>
</ul>
<p>$$
\mathbf{H}^{-1} = (\mathbf{L} \mathbf{L}^\top )^{-1}
$$</p>
<p>However, we don&rsquo;t compute $ \mathbf{H}^{-1} $ explicitly. Instead, we use $ \mathbf{L} $ to solve systems involving $ \mathbf{H}^{-1} $.</p>
<h4 id="62-quantization-loop"><strong>6.2. Quantization Loop</strong><a hidden class="anchor" aria-hidden="true" href="#62-quantization-loop">#</a></h4>
<p>For each block of columns (weights), perform the following:</p>
<p><strong>Step 1: Quantize Weights</strong></p>
<ul>
<li>For each column $ j $ in the block, quantize $ \mathbf{W}_{:, j} $:</li>
</ul>
<p>$$
\mathbf{Q}<em>{:, j} = \text{quant}(\mathbf{W}</em>{:, j})
$$</p>
<ul>
<li>Compute quantization error:</li>
</ul>
<p>$$
\delta \mathbf{W}<em>{:, j} = \mathbf{W}</em>{:, j} - \mathbf{Q}_{:, j}
$$</p>
<p><strong>Step 2: Update Remaining Weights</strong></p>
<ul>
<li>
<p>Adjust unquantized weights to compensate for the error introduced by quantizing the current weight(s).</p>
</li>
<li>
<p>Use the Cholesky factors to compute the necessary adjustments efficiently.</p>
</li>
</ul>
<p><strong>Step 3: Batch Updates</strong></p>
<ul>
<li>After processing the block, update the remaining weights and the relevant parts of $ \mathbf{H}^{-1} $.</li>
</ul>
<p><strong>Algorithm Pseudocode</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>Initialize Q = zeros(d_row, d_col)
</span></span><span style="display:flex;"><span>Compute H = 2 * X * X^T + lambda * I
</span></span><span style="display:flex;"><span>Compute Cholesky decomposition of H^-1: H_inv = Cholesky(H^-1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>For i in range(0, d_col, block_size):
</span></span><span style="display:flex;"><span>    For j in range(i, i + block_size):
</span></span><span style="display:flex;"><span>        Q[:, j] = quantize(W[:, j])
</span></span><span style="display:flex;"><span>        E[:, j - i] = (W[:, j] - Q[:, j]) / H_inv[j, j]
</span></span><span style="display:flex;"><span>        W[:, j:(i + block_size)] -= E[:, j - i] * H_inv[j, j:(i + block_size)]
</span></span><span style="display:flex;"><span>    W[:, (i + block_size):] -= E * H_inv[i:(i + block_size), (i + block_size):]
</span></span></code></pre></div><hr>
<h3 id="7-example"><strong>7. Example</strong><a hidden class="anchor" aria-hidden="true" href="#7-example">#</a></h3>
<p>Let&rsquo;s illustrate the GPTQ algorithm with a simplified example.</p>
<p><strong>Assumptions</strong>:</p>
<ul>
<li>$ \mathbf{W} $ is a $ 2 \times 2 $ weight matrix.</li>
<li>$ \mathbf{X} $ is a $ 2 \times 3 $ input matrix (3 examples).</li>
<li>We quantize to 1-bit weights (e.g., $-1$ or $+1$).</li>
</ul>
<p><strong>Step 1: Initialize</strong></p>
<ul>
<li><strong>Weights</strong>:</li>
</ul>
<p>$$
\mathbf{W} = \begin{bmatrix} 0.8 &amp; -0.5 \ 0.3 &amp; 0.7 \end{bmatrix}
$$</p>
<ul>
<li><strong>Inputs</strong>:</li>
</ul>
<p>$$
\mathbf{X} = \begin{bmatrix} 0.2 &amp; -0.1 &amp; 0.4 \ 0.5 &amp; 0.3 &amp; -0.2 \end{bmatrix}
$$</p>
<ul>
<li><strong>Compute Hessian</strong>:</li>
</ul>
<p>$$
\mathbf{H} = 2 \mathbf{X} \mathbf{X}^\top + \lambda \mathbf{I}
$$</p>
<p>Compute $ \mathbf{X} \mathbf{X}^\top $:</p>
<p>$$
\mathbf{X} \mathbf{X}^\top = \begin{bmatrix} (0.2)^2 + (-0.1)^2 + (0.4)^2 &amp; 0.2*0.5 + (-0.1)<em>0.3 + 0.4</em>(-0.2) \ \text{Symmetric} &amp; (0.5)^2 + (0.3)^2 + (-0.2)^2 \end{bmatrix}
$$</p>
<p>Compute $ \mathbf{H} $ (assuming $ \lambda = 0 $ for simplicity).</p>
<ul>
<li><strong>Cholesky Decomposition</strong>:</li>
</ul>
<p>Compute $ \mathbf{L} $ such that $ \mathbf{H} = \mathbf{L} \mathbf{L}^\top $.</p>
<p><strong>Step 2: Quantization</strong></p>
<ul>
<li><strong>Quantize $ \mathbf{W} $</strong>:</li>
</ul>
<p>$$
\mathbf{Q} = \begin{bmatrix} \text{quant}(0.8) &amp; \text{quant}(-0.5) \ \text{quant}(0.3) &amp; \text{quant}(0.7) \end{bmatrix} = \begin{bmatrix} 1 &amp; -1 \ 1 &amp; 1 \end{bmatrix}
$$</p>
<ul>
<li><strong>Compute Quantization Error</strong>:</li>
</ul>
<p>$$
\delta \mathbf{W} = \mathbf{W} - \mathbf{Q} = \begin{bmatrix} 0.8 - 1 &amp; -0.5 + 1 \ 0.3 - 1 &amp; 0.7 - 1 \end{bmatrix} = \begin{bmatrix} -0.2 &amp; 0.5 \ -0.7 &amp; -0.3 \end{bmatrix}
$$</p>
<p><strong>Step 3: Update Remaining Weights</strong></p>
<ul>
<li>
<p>Use $ \delta \mathbf{W} $ and $ \mathbf{H}^{-1} $ (via Cholesky factors) to adjust the unquantized weights.</p>
</li>
<li>
<p>For this small example, adjustments are minor.</p>
</li>
</ul>
<hr>
<h3 id="8-understanding-the-hessian-in-pre-trained-models"><strong>8. Understanding the Hessian in Pre-trained Models</strong><a hidden class="anchor" aria-hidden="true" href="#8-understanding-the-hessian-in-pre-trained-models">#</a></h3>
<h4 id="81-source-of-the-hessian"><strong>8.1. Source of the Hessian</strong><a hidden class="anchor" aria-hidden="true" href="#81-source-of-the-hessian">#</a></h4>
<p>In the context of quantization, the Hessian matrix $ \mathbf{H} $ arises from the second-order Taylor expansion of the error function with respect to the weights.</p>
<ul>
<li><strong>Error Function</strong>:</li>
</ul>
<p>$$
E(\mathbf{w}_c) = | \mathbf{w}\mathbf{X} - \mathbf{w}_c \mathbf{X} |_2^2
$$</p>
<ul>
<li><strong>First Derivative</strong>:</li>
</ul>
<p>$$
\frac{\partial E}{\partial \mathbf{w}_c} = -2 (\mathbf{w}\mathbf{X} - \mathbf{w}_c \mathbf{X}) \mathbf{X}^\top
$$</p>
<ul>
<li><strong>Second Derivative (Hessian)</strong>:</li>
</ul>
<p>$$
\mathbf{H} = \frac{\partial^2 E}{\partial \mathbf{w}_c^2} = 2 \mathbf{X} \mathbf{X}^\top
$$</p>
<h4 id="82-interpretation"><strong>8.2. Interpretation</strong><a hidden class="anchor" aria-hidden="true" href="#82-interpretation">#</a></h4>
<ul>
<li>
<p>$ \mathbf{H} $ captures how sensitive the error is to changes in $ \mathbf{w}_c $.</p>
</li>
<li>
<p>In quantization, we are interested in how quantizing a weight affects the overall error, and $ \mathbf{H} $ provides this information.</p>
</li>
</ul>
<h4 id="83-hessian-computation-in-practice"><strong>8.3. Hessian Computation in Practice</strong><a hidden class="anchor" aria-hidden="true" href="#83-hessian-computation-in-practice">#</a></h4>
<ul>
<li>
<p>For large models, computing $ \mathbf{H} $ directly is impractical.</p>
</li>
<li>
<p><strong>Approximation</strong>: Use a subset of data ($ m $ examples) to compute $ \mathbf{X} $ and hence $ \mathbf{H} $.</p>
</li>
<li>
<p><strong>Regularization</strong>: Add damping ($ \lambda \mathbf{I} $) to $ \mathbf{H} $ to ensure it is positive-definite and invertible.</p>
</li>
</ul>
<hr>
<h3 id="9-cholesky-decomposition-in-detail"><strong>9. Cholesky Decomposition in Detail</strong><a hidden class="anchor" aria-hidden="true" href="#9-cholesky-decomposition-in-detail">#</a></h3>
<h4 id="91-mathematical-background"><strong>9.1. Mathematical Background</strong><a hidden class="anchor" aria-hidden="true" href="#91-mathematical-background">#</a></h4>
<ul>
<li><strong>Definition</strong>: For a symmetric positive-definite matrix $ \mathbf{A} $, there exists a unique lower triangular matrix $ \mathbf{L} $ with positive diagonal elements such that:</li>
</ul>
<p>$$
\mathbf{A} = \mathbf{L} \mathbf{L}^\top
$$</p>
<h4 id="92-computation-steps"><strong>9.2. Computation Steps</strong><a hidden class="anchor" aria-hidden="true" href="#92-computation-steps">#</a></h4>
<ol>
<li>
<p><strong>Initialization</strong>:</p>
<ul>
<li>Let $ \mathbf{A} $ be $ n \times n $.</li>
<li>$ \mathbf{L} $ is initialized as a zero matrix.</li>
</ul>
</li>
<li>
<p><strong>Algorithm</strong>:</p>
<p>For $ i = 1 $ to $ n $:</p>
<ul>
<li>
<p>Compute:</p>
<p>$$
L_{ii} = \sqrt{A_{ii} - \sum_{k=1}^{i-1} L_{ik}^2}
$$</p>
</li>
<li>
<p>For $ j = i+1 $ to $ n $:</p>
<p>$$
L_{ji} = \frac{1}{L_{ii}} \left( A_{ji} - \sum_{k=1}^{i-1} L_{jk} L_{ik} \right)
$$</p>
</li>
</ul>
</li>
<li>
<p><strong>Result</strong>:</p>
<ul>
<li>$ \mathbf{L} $ is lower triangular.</li>
<li>$ \mathbf{L} \mathbf{L}^\top = \mathbf{A} $.</li>
</ul>
</li>
</ol>
<h4 id="93-significance-in-gptq"><strong>9.3. Significance in GPTQ</strong><a hidden class="anchor" aria-hidden="true" href="#93-significance-in-gptq">#</a></h4>
<ul>
<li>
<p><strong>Numerical Stability</strong>: Cholesky decomposition is numerically stable for positive-definite matrices.</p>
</li>
<li>
<p><strong>Efficient Solves</strong>: Allows solving linear systems $ \mathbf{A}\mathbf{x} = \mathbf{b} $ by:</p>
<ol>
<li>Forward substitution to solve $ \mathbf{L}\mathbf{y} = \mathbf{b} $.</li>
<li>Backward substitution to solve $ \mathbf{L}^\top \mathbf{x} = \mathbf{y} $.</li>
</ol>
</li>
<li>
<p><strong>Avoids Explicit Inversion</strong>: Inverting $ \mathbf{H} $ directly can be numerically unstable and computationally expensive.</p>
</li>
</ul>
<hr>
<h3 id="10-conclusion"><strong>10. Conclusion</strong><a hidden class="anchor" aria-hidden="true" href="#10-conclusion">#</a></h3>
<p>GPTQ is a powerful algorithm for quantizing large neural networks efficiently while maintaining high accuracy. By leveraging insights about quantization order, batching updates, and utilizing Cholesky decomposition, GPTQ addresses the computational and numerical challenges posed by large-scale models.</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>
<p><strong>Hessian Matrix</strong>: Central to understanding how quantization errors propagate and how to adjust weights to minimize the overall error.</p>
</li>
<li>
<p><strong>Cholesky Decomposition</strong>: A numerically stable method to work with the Hessian inverse without explicit inversion, crucial for large models.</p>
</li>
<li>
<p><strong>Algorithm Efficiency</strong>: GPTQ&rsquo;s design reduces computational complexity, making it practical for models with billions of parameters.</p>
</li>
</ul>
<p>By understanding the mathematical foundations and practical implementations, we can appreciate the advancements GPTQ brings to the field of neural network quantization.</p>
<hr>
<p><strong>References</strong>:</p>
<ul>
<li>Frantar, E., &amp; Alistarh, D. (2022). Optimal Brain Quantization. <em>Proceedings of the International Conference on Learning Representations (ICLR)</em>.</li>
<li>Nagel, M., Van Baalen, M., Blankevoort, T., &amp; Welling, M. (2020). Up or Down? Adaptive Rounding for Post-Training Quantization. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://localhost:34579/">TensorTunes</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
