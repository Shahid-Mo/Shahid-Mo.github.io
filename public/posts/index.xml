<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on TensorTunes</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on TensorTunes</description>
    <generator>Hugo -- 0.129.0</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Where Did All the Memory Go?</title>
      <link>http://localhost:1313/posts/where_v3/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/where_v3/</guid>
      <description>CUDA error: out of memory
If you&amp;rsquo;ve ever tried to train a deep learning model, the dreaded CUDA error: out of memory is likely all too familiar. The usual quick fix is to decrease the batch size and move on without giving it much thought. But have you ever wondered about how memory gets allocated during training?? In this blog post, I want to demystify memory consumption during model training and and offer practical methods to reduce the demands of memory-heavy models.</description>
    </item>
    <item>
      <title>Quantization in LLMS (Part 1): LLM.int8(), NF4</title>
      <link>http://localhost:1313/posts/quantization/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/quantization/</guid>
      <description>Introduction to Quantization Whether you&amp;rsquo;re an AI enthusiast looking to run large language models (LLMs) on your personal device, a startup aiming to serve state-of-the-art models efficiently, or a researcher fine-tuning models for specific tasks, quantization is a key technique to understand.
Quantization can be broadly categorized into two main approaches:
Quantization Aware Training (QAT): This involves training the model with reduced precision, allowing it to adjust during the training process to perform well under quantized conditions.</description>
    </item>
  </channel>
</rss>
