<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=39705&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Quantization in LLMS (Part 1): LLM.int8(), NF4 | TensorTunes</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction to Quantization Whether you&rsquo;re an AI enthusiast looking to run large language models (LLMs) locally on your personal device, a multi-billion-dollar startup aiming to serve a state-of-the-art model to customers, or someone wanting to fine-tune models like LLaMA or Flux, quantization is a technique you need to understand.
Quantization can broadly be categorized into two types:
Quantization Aware Training (QAT): Integrates quantization into the training process, allowing the model to adapt to reduced precision during training.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:39705/posts/quantization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:39705/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:39705/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:39705/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:39705/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:39705/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:39705/posts/quantization/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:39705/" accesskey="h" title="TensorTunes (Alt + H)">TensorTunes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:39705/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:39705/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Quantization in LLMS (Part 1): LLM.int8(), NF4
    </h1>
    <div class="post-meta"><span title='2024-09-11 00:00:00 +0000 UTC'>September 11, 2024</span>

</div>
  </header> 
  <div class="post-content"><h2 id="introduction-to-quantization">Introduction to Quantization<a hidden class="anchor" aria-hidden="true" href="#introduction-to-quantization">#</a></h2>
<p>Whether you&rsquo;re an AI enthusiast looking to run large language models (LLMs) locally on your personal device, a multi-billion-dollar startup aiming to serve a state-of-the-art model to customers, or someone wanting to fine-tune models like LLaMA or Flux, <strong>quantization</strong> is a technique you need to understand.</p>
<p>Quantization can broadly be categorized into two types:</p>
<ul>
<li>
<p><strong>Quantization Aware Training (QAT):</strong> Integrates quantization into the training process, allowing the model to adapt to reduced precision during training.</p>
</li>
<li>
<p><strong>Post Training Quantization (PTQ):</strong> Applies quantization after a model is already trained, focusing on reducing its size and inference cost without retraining.</p>
</li>
</ul>
<p>In this post (and subsequent ones on this topic), we&rsquo;ll focus on Post Training Quantization (PTQ) — a method used to make trained models smaller and more efficient, particularly useful when deploying models to edge devices or serving models at lower hardware costs.</p>
<h2 id="why-quantization">Why Quantization?<a hidden class="anchor" aria-hidden="true" href="#why-quantization">#</a></h2>
<p>Consider this: the costs of training an LLM are already high, but inference costs — running the model to generate responses — can far exceed training costs, especially when deploying at scale. For example, inference costs for models like ChatGPT can surpass training costs within a week. Quantization reduces these costs by allowing models to operate in lower precision, such as FP16 or even FP8, without significant performance loss.</p>
<p>Let’s start with a real-world example using the GPT-2 model from Hugging Face. We will quantize it to reduce its memory footprint and discuss how to efficiently convert FP32 to lower-precision formats like FP16 and beyond.</p>
<p>We will begin with some &ldquo;back-of-the-napkin&rdquo; calculations. Suppose you want to run the LLaMA 3.1 70B model. What kind of memory and GPU are you looking at? If you load the model from Hugging Face, it will automatically be loaded in full precision (FP32). Just to load the model weights, you will require:
$$
70 \times 10^9 \text{ parameters} \times 32 \text{ bits per parameter} \div 8 \div 1024^3 \approx 280 \text{ GB}
$$</p>
<p>So, you would need at least an H100 GPU with 80 GB of VRAM to load the model, and you wouldn&rsquo;t even be able to perform inference or fine-tuning. You would likely need to get a cluster for that.</p>
<p>This is where quantization can help you. By loading the model in 8-bit, 4-bit, or even 2-bit precision, you can reduce your memory requirements by a factor of up to 16. If you load the same model in NF4 format (4 bits per parameter), you would need just around 35 GB, which can be accomplished even on the free-tier T4 Colab.</p>
<h2 id="understanding-precision-in-llms">Understanding Precision in LLMs<a hidden class="anchor" aria-hidden="true" href="#understanding-precision-in-llms">#</a></h2>
<p>If you&rsquo;re not familiar with terms like FP32 or 8-bit precision from the previous section, we&rsquo;re going to cover that next.</p>
<h3 id="starting-with-fp32">Starting with FP32<a hidden class="anchor" aria-hidden="true" href="#starting-with-fp32">#</a></h3>
<p>FP32 (32-bit floating point) is the most common datatype used to train deep learning models. If you don’t specify the datatypes of tensors in PyTorch, FP32 is the default datatype.</p>
<p>To understand how FP32 works and why it is often preferred, let’s look at a simple example: representing $\pi$ (pi) up to 10 decimal places.</p>
<h3 id="precision-in-deep-learning-models">Precision in Deep Learning Models<a hidden class="anchor" aria-hidden="true" href="#precision-in-deep-learning-models">#</a></h3>
<p>Let&rsquo;s assume we want to represent $\pi$ with the first 10 decimal places:</p>
<p>$$
\pi \approx 3.1415926535
$$</p>
<p>While this example is quite basic, it helps illustrate the inner workings of different data types, such as BF16 and FP16 — and in future posts, FP8 (which is supported by the latest NVIDIA H100 GPUs and similar hardware).</p>
<h1 id="how-to-convert-a-decimal-representation-to-fp32">How to Convert a Decimal Representation to FP32<a hidden class="anchor" aria-hidden="true" href="#how-to-convert-a-decimal-representation-to-fp32">#</a></h1>
<p>To convert the decimal value of $\pi$ (3.1415926535) into its FP32 (single-precision floating-point) representation, we need to follow these steps:</p>
<h2 id="1-understand-the-fp32-structure">1. Understand the FP32 Structure<a hidden class="anchor" aria-hidden="true" href="#1-understand-the-fp32-structure">#</a></h2>
<p>FP32 is composed of 32 bits, divided into three parts:</p>
<ul>
<li><strong>Sign bit (1 bit):</strong> Indicates whether the number is positive or negative.</li>
<li><strong>Exponent (8 bits):</strong> Encodes the exponent using a biased format.</li>
<li><strong>Mantissa (23 bits):</strong> Represents the significant digits of the number.</li>
</ul>
<h2 id="2-convert-pi-to-binary">2. Convert $\pi$ to Binary<a hidden class="anchor" aria-hidden="true" href="#2-convert-pi-to-binary">#</a></h2>
<p>We start by converting the decimal value of $\pi$ into its binary equivalent.</p>
<p><strong>Convert the Integer Part:</strong></p>
<p>The integer part of $\pi$ is 3. In binary:</p>
<p>$$
3_{10} = 11_{2}
$$</p>
<p><strong>Convert the Fractional Part:</strong></p>
<p>To convert the fractional part (0.1415926535) to binary:</p>
<ul>
<li>Multiply by 2 and record the integer part repeatedly:
<ul>
<li>$0.1415926535 \times 2 = 0.283185307 \rightarrow 0$</li>
<li>$0.283185307 \times 2 = 0.566370614 \rightarrow 0$</li>
<li>$0.566370614 \times 2 = 1.132741228 \rightarrow 1$</li>
<li>$0.132741228 \times 2 = 0.265482456 \rightarrow 0$</li>
<li>$0.265482456 \times 2 = 0.530964912 \rightarrow 0$</li>
<li>Continue this process to get more bits.</li>
</ul>
</li>
</ul>
<p>The binary representation of $\pi$ up to a reasonable precision is approximately:</p>
<p>$$
\pi \approx 11.001001000011111101101010100010_{2}
$$</p>
<h2 id="3-normalize-the-binary-representation">3. Normalize the Binary Representation<a hidden class="anchor" aria-hidden="true" href="#3-normalize-the-binary-representation">#</a></h2>
<p>To fit the FP32 format, normalize the binary number so that it appears as:</p>
<p>$$
11.001001000011111101101010100010_{2} = 1.1001001000011111101101010100010_{2} \times 2^1
$$</p>
<h2 id="4-determine-the-sign-bit">4. Determine the Sign Bit<a hidden class="anchor" aria-hidden="true" href="#4-determine-the-sign-bit">#</a></h2>
<p>Since $\pi$ is positive, the sign bit is:</p>
<p>$$
\text{Sign bit} = 0
$$</p>
<h2 id="5-calculate-the-exponent">5. Calculate the Exponent<a hidden class="anchor" aria-hidden="true" href="#5-calculate-the-exponent">#</a></h2>
<p>The exponent is stored with a bias of 127 in FP32. This bias allows the representation of both very large and very small numbers. In FP32, the largest number you can represent is approximately $3.4 \times 10^{38}$, while the smallest positive number (close to zero) is around $1.4 \times 10^{-45}$.</p>
<p>For $\pi$, the exponent after normalization is 1:</p>
<p>$$
\text{Exponent} = 1 + 127 = 128
$$</p>
<p>The exponent in binary is:</p>
<p>$$
128_{10} = 10000000_{2}
$$</p>
<h2 id="6-determine-the-mantissa">6. Determine the Mantissa<a hidden class="anchor" aria-hidden="true" href="#6-determine-the-mantissa">#</a></h2>
<p>The mantissa consists of the significant digits after the leading 1:</p>
<p>$$
\text{Mantissa} = 10010010000111111011010_{2}
$$</p>
<p>Only the first 23 bits are retained, and any extra bits are truncated, resulting in some loss of precision. FP32 typically provides around 7 to 8 decimal places of precision. If you require more precision, you could use FP64 (double-precision), but for deep learning, FP32 is often more than sufficient.</p>
<h2 id="7-combine-the-components">7. Combine the Components<a hidden class="anchor" aria-hidden="true" href="#7-combine-the-components">#</a></h2>
<p>Combine the sign bit, exponent, and mantissa to form the final FP32 representation:</p>
<ul>
<li><strong>Sign bit:</strong> 0</li>
<li><strong>Exponent:</strong> 10000000</li>
<li><strong>Mantissa:</strong> 10010010000111111011010</li>
</ul>
<p>Thus, the FP32 representation of $\pi$ is:</p>
<p>$$
\text{FP32} = 0\ 10000000\ 10010010000111111011010_{2}
$$</p>
<p>This is the IEEE 754 standard representation of $\pi$ in FP32 format.</p>
<h1 id="python-code-to-demonstrate-fp32-and-other-precisions">Python Code to Demonstrate FP32 and Other Precisions<a hidden class="anchor" aria-hidden="true" href="#python-code-to-demonstrate-fp32-and-other-precisions">#</a></h1>
<p>Let&rsquo;s use Python to compare different floating-point representations of $\pi$ and calculate their precision errors:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Pi value up to 10 digits</span>
</span></span><span style="display:flex;"><span>pi_val <span style="color:#f92672">=</span> <span style="color:#ae81ff">3.1415926535</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># FP32 (single precision)</span>
</span></span><span style="display:flex;"><span>pi_fp32 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(pi_val, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># BF16 (Brain Floating Point 16, half precision)</span>
</span></span><span style="display:flex;"><span>pi_bf16 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(pi_val, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>bfloat16)
</span></span></code></pre></div><pre tabindex="0"><code>og:   pi = 3.1415926535
FP32: Pi = 3.1415927410125732, Error % = 0.0000027856%
BF16: Pi = 3.140625,           Error % = 0.0308013675%
</code></pre><h3 id="fp8">FP8<a hidden class="anchor" aria-hidden="true" href="#fp8">#</a></h3>
<p>Recent research, such as the &ldquo;FP8 FORMATS FOR DEEP LEARNING&rdquo; paper from September 2022, even shows that it&rsquo;s possible to train large language models (LLMs) using FP8 with just 2 or 3 bits of mantissa (depending on the E4M3 or E5M2 format).</p>
<figure class="center">
    <img loading="lazy" src="/images/quant_p1/fp8_vs_bf16.png"
         alt="FP8 vs BF16 comparison"/> <figcaption>
            <p>Figure 1: Comparison of FP8 and BF16 formats. Source: <a href="https://arxiv.org/abs/xxxx.xxxxx">Smith et al. (2023)</a></p>
        </figcaption>
</figure>

<p>Here’s the table formatted in Markdown for your blog, which you can directly use if your blog platform supports Markdown syntax:</p>
<h3 id="gpu-performance-table">GPU Performance Table<a hidden class="anchor" aria-hidden="true" href="#gpu-performance-table">#</a></h3>
<table>
<thead>
<tr>
<th>GPU</th>
<th>P100</th>
<th>V100</th>
<th>T4</th>
<th>A100</th>
<th>H100</th>
<th>L40</th>
<th>B100</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>VRAM</strong></td>
<td>16GB</td>
<td>32GB</td>
<td>16GB</td>
<td>40GB*</td>
<td>80GB</td>
<td>48GB</td>
<td>192GB</td>
</tr>
<tr>
<td><strong>Architecture</strong></td>
<td>Pascal</td>
<td>Volta</td>
<td>Turing</td>
<td>Ampere</td>
<td>Hopper</td>
<td>Ada Lovelace</td>
<td>Blackwell</td>
</tr>
<tr>
<td><strong>Release Date</strong></td>
<td>Apr-16</td>
<td>May-17</td>
<td>Sep-18</td>
<td>May-20</td>
<td>Mar-22</td>
<td>Sep-22</td>
<td>Mar-24</td>
</tr>
<tr>
<td><strong>FP32</strong></td>
<td>10.6</td>
<td>8.2</td>
<td>8.1</td>
<td>19.5</td>
<td>67</td>
<td>90.5</td>
<td>60</td>
</tr>
<tr>
<td><strong>TF32</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>312</td>
<td>989</td>
<td>181</td>
<td>1800</td>
</tr>
<tr>
<td><strong>FP16</strong></td>
<td></td>
<td>130</td>
<td>65</td>
<td>624</td>
<td>1979</td>
<td>362</td>
<td>3500</td>
</tr>
<tr>
<td><strong>BF16</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>624</td>
<td>1979</td>
<td>362</td>
<td>3500</td>
</tr>
<tr>
<td><strong>INT8</strong></td>
<td>X</td>
<td>X</td>
<td>130</td>
<td>1248</td>
<td>3958</td>
<td>724</td>
<td>7000</td>
</tr>
<tr>
<td><strong>INT4</strong></td>
<td>X</td>
<td>X</td>
<td>260</td>
<td>2496</td>
<td></td>
<td>1448</td>
<td></td>
</tr>
<tr>
<td><strong>FP8</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>3958</td>
<td>724</td>
<td>7000</td>
</tr>
<tr>
<td><strong>FP4</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>14000</td>
</tr>
</tbody>
</table>
<h3 id="how-to-use-this-table-in-your-blog">How to Use This Table in Your Blog<a hidden class="anchor" aria-hidden="true" href="#how-to-use-this-table-in-your-blog">#</a></h3>
<ul>
<li><strong>Headers</strong> are defined with the <code>|</code> symbol.</li>
<li><strong>Values</strong> are centered by using colons <code>:</code> at the start and end of the hyphens <code>---</code> for each column.</li>
<li><strong>Cells marked with &lsquo;X&rsquo;</strong> indicate the data type is not supported by that GPU.</li>
</ul>
<h4 id="additional-formatting-tips">Additional Formatting Tips:<a hidden class="anchor" aria-hidden="true" href="#additional-formatting-tips">#</a></h4>
<ul>
<li>To make this table &ldquo;dank&rdquo; or visually appealing, consider using a custom stylesheet for your blog or platform. You can add background colors, borders, or hover effects using CSS.</li>
<li><strong>Use a monospaced font</strong> for the table or apply a dark theme to your blog to match the style.</li>
</ul>
<p>Feel free to copy and paste this Markdown into your blog editor!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://localhost:39705/">TensorTunes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
