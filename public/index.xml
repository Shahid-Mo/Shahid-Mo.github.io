<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>TensorTunes</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on TensorTunes</description>
    <generator>Hugo -- 0.135.0</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deconding From Language Models</title>
      <link>http://localhost:1313/posts/decoding-from-language-models/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/decoding-from-language-models/</guid>
      <description>&lt;h2 id=&#34;a-quick-refresher-on-autoregressive-text-generation&#34;&gt;A quick refresher on Autoregressive text generation&lt;/h2&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/images/Decoding/decoding_1.png&#34; alt=&#34;TF32 Explained&#34; style=&#34;display: block; margin: 0 auto;&#34;&gt;
&lt;p style=&#34;font-size: 0.8em; color: rgba(0, 0, 0, 0.6);&#34;&gt;
  Figure 1: Comparison of FP8 and BF16 formats. Source: 
  &lt;a href=&#34;https://arxiv.org/abs/xxxx.xxxxx&#34; style=&#34;color: rgba(0, 0, 0, 0.6);&#34;&gt;Smith et al. (2023)&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Autoregressive language models generate text through a sequential process of predicting one token at a time. The model takes a sequence of tokens $ \lbrace y \rbrace _{&amp;lt;t} $ as input  and outputs a new token $ \hat{y_t} $. This process repeats iteratively, with each newly generated token becoming part of the input for the subsequent prediction.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Retrieval to RAG (Part - 1)</title>
      <link>http://localhost:1313/posts/from-retrieval-to-rag-part1/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/from-retrieval-to-rag-part1/</guid>
      <description>&lt;h2 id=&#34;introduction-to-quantization&#34;&gt;Introduction to Quantization&lt;/h2&gt;
&lt;p&gt;Whether you&amp;rsquo;re an AI enthusiast looking to run large language models (LLMs) on your personal device, a startup aiming to serve state-of-the-art models efficiently, or a researcher fine-tuning models for specific tasks, &lt;strong&gt;quantization&lt;/strong&gt; is a key technique to understand.&lt;/p&gt;
&lt;p&gt;Quantization can be broadly categorized into two main approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quantization Aware Training (QAT):&lt;/strong&gt; This involves training the model with reduced precision, allowing it to adjust during the training process to perform well under quantized conditions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Post Training Quantization (PTQ):&lt;/strong&gt; Applied after a model has already been trained, PTQ reduces model size and inference cost without needing to retrain, making it especially useful for deploying models efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post (and subsequent ones), we&amp;rsquo;ll focus on PTQ. It&amp;rsquo;s often a simpler starting point for quantization, providing a good balance between performance and implementation complexity. PTQ is particularly useful for deploying models to edge devices or serving them at lower hardware costs, making it an excellent first step in your quantization journey.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parameter Efficient Fine-tuning of LLMs (PEFT)</title>
      <link>http://localhost:1313/posts/pfet/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/pfet/</guid>
      <description>&lt;h1 id=&#34;motivation-for-peft&#34;&gt;Motivation for PEFT&lt;/h1&gt;
&lt;p&gt;Consider a company like character.ai, which provides different personas for users. For example, you can talk to a chat bot that mimics Elon Musk and ask, &amp;ldquo;Why did you buy Twitter?&amp;rdquo; The model responds as Elon Musk would.&lt;/p&gt;
&lt;p&gt;Now there are primarily three approaches to solving this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Context-based approach:&lt;/strong&gt; Take an LLM and provide it with extensive data about the persona (e.g., Elon Musk&amp;rsquo;s interviews and tweets) as context, and then tag on your question. This method is not the most elegant or efficient way to approach the problem and may struggle with persona consistency. Another issue is of the context length, LLMs have a limited context length and we might have more data that can fit into the context length of the model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantization in LLMS (Part 1): LLM.int8(), NF4</title>
      <link>http://localhost:1313/posts/quantization/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/quantization/</guid>
      <description>&lt;h1 id=&#34;introduction-to-quantization&#34;&gt;Introduction to Quantization&lt;/h1&gt;
&lt;p&gt;Whether you&amp;rsquo;re an AI enthusiast looking to run large language models (LLMs) on your personal device, a startup aiming to serve state-of-the-art models efficiently, or a researcher fine-tuning models for specific tasks, &lt;strong&gt;quantization&lt;/strong&gt; is a key technique to understand.&lt;/p&gt;
&lt;p&gt;Quantization can be broadly categorized into two main approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quantization Aware Training (QAT):&lt;/strong&gt; This involves training the model with reduced precision, allowing it to adjust during the training process to perform well under quantized conditions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Post Training Quantization (PTQ):&lt;/strong&gt; Applied after a model has already been trained, PTQ reduces model size and inference cost without needing to retrain, making it especially useful for deploying models efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post (and subsequent ones on this topic), we&amp;rsquo;ll focus on PTQ. It&amp;rsquo;s often a simpler starting point for quantization, providing a good balance between performance and implementation complexity. PTQ is particularly useful for deploying models to edge devices or serving them at lower hardware costs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantization in LLMS Part 2: GPTQ (A Mathematical View)</title>
      <link>http://localhost:1313/posts/gptq/</link>
      <pubDate>Thu, 01 Aug 2024 09:51:17 -0400</pubDate>
      <guid>http://localhost:1313/posts/gptq/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Quantization is a crucial technique in deep learning that reduces the memory footprint and computational requirements of neural networks by representing weights and activations with lower-precision numerical formats. This is particularly important when deploying large models on devices with limited resources. However, quantizing a neural network without significantly degrading its performance is challenging.&lt;/p&gt;
&lt;p&gt;The GPTQ (Gradient Post-Training Quantization) algorithm is a method designed to efficiently quantize large-scale neural networks, such as those used in natural language processing, while maintaining high accuracy. GPTQ builds upon previous methods like Optimal Brain Quantization (OBQ) but introduces significant modifications to make it scalable to models with billions of parameters.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
