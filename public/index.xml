<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>TensorTunes</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on TensorTunes</description>
    <generator>Hugo -- 0.129.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Reasoning [Draft]</title>
      <link>http://localhost:1313/posts/llm_reasoning/</link>
      <pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/llm_reasoning/</guid>
      <description>Motivation for Reasoning Classical ML algorithms like Linear Regression, Support Vector Machines, K- Nearest Neighbours etc, have long served as powerful tools for narrow, well-structured problems, but what sets apaprt todays field of AI that distinguishes itself from the previous generation is the ability of todays models to reason (or do sometinig that resembels reasonig, quite convincingly i might add), the ideal of AGI (artificial general intellingence) is a computer that can reason and plan ahead, and current llms can be taught to do many things, this just comes under the paradigm of prompt engineering.</description>
    </item>
    <item>
      <title>Where Did All the Memory Go?</title>
      <link>http://localhost:1313/posts/where_v3/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/where_v3/</guid>
      <description>CUDA error: out of memory
If you&amp;rsquo;ve ever tried to train a deep learning model, the dreaded CUDA error: out of memory is likely all too familiar. The usual quick fix is to decrease the batch size and move on without giving it much thought. But have you ever wondered about how memory gets allocated during training?? In this blog post, I want to demystify memory consumption during model training and and offer practical methods to reduce the demands of memory-heavy models.</description>
    </item>
    <item>
      <title>RLHF: PPO [Draft]</title>
      <link>http://localhost:1313/posts/rlhf_v2/</link>
      <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/rlhf_v2/</guid>
      <description>Reinforcement Learning from Human Feedback (RLHF): Aligning LLMs with Human Intent Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique in the advancement of large language models (LLMs), aiming to align their behavior more closely with human intentions and ethical standards. While starting with a large pretrained LLM—such as LLaMA 2 with its 7 billion parameters trained on a trillion tokens—provides a strong foundation, these models can still struggle with handling harmful or toxic queries effectively.</description>
    </item>
    <item>
      <title>Multi_GPU_Training [Draft]</title>
      <link>http://localhost:1313/posts/multi_gpu_training/</link>
      <pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/multi_gpu_training/</guid>
      <description>In this post, i wont be discussing code implementations, my goal is to cover the foundational concepts related to multi-GPU Training of Massive llms,
as stated in my post on Qunatization, you would need a cluster of gpus just to get up and running with the finetuning of of even small llms like the llama 7B models.
The topics i would like to cover are as follows
DDP (Distributed Data Parallel) Tensor Model parallelism Pipeline model parallelism Memory efficient pipeline parallelism Lest start Multi GPU Training</description>
    </item>
    <item>
      <title>Decoding From Language Models</title>
      <link>http://localhost:1313/posts/decoding-from-language-models/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/decoding-from-language-models/</guid>
      <description>A quick refresher on Autoregressive text generation Autoregressive language models generate text through a sequential process of predicting one token at a time. The model takes a sequence of tokens $ \lbrace y \rbrace _{&amp;lt;t} $ as input and outputs a new token $ \hat{y_t} $. This process repeats iteratively, with each newly generated token becoming part of the input for the subsequent prediction.
At each time step $ t $, the model computes a vector of scores $ \mathbf{S} \in \mathbb{R}^V $, where $ V $ is the size of the vocabulary.</description>
    </item>
    <item>
      <title>From Retrieval to RAG (Part - 1)</title>
      <link>http://localhost:1313/posts/from-retrieval-to-rag-part1/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/from-retrieval-to-rag-part1/</guid>
      <description>ChatGPT can make mistakes. Check important info. This disclaimer appears beneath the input field in ChatGPT and is not unique to it—similar notices can be found across all major large language models (LLMs). This is because one of the most well-known issues with LLMs is their tendency to hallucinate, meaning they can generate information that isn&amp;rsquo;t accurate or grounded in reality. So, before submitting your history paper directly from ChatGPT, make sure to proofread it carefully.</description>
    </item>
    <item>
      <title>Parameter Efficient Fine-tuning of LLMs (PEFT) [Draft]</title>
      <link>http://localhost:1313/posts/pfet/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/pfet/</guid>
      <description>Motivation for PEFT Consider a company like character.ai, which provides different personas for users. For example, you can talk to a chat bot that mimics Elon Musk and ask, &amp;ldquo;Why did you buy Twitter?&amp;rdquo; The model responds as Elon Musk would.
Now there are primarily three approaches to solving this:
Context-based approach: Take an LLM and provide it with extensive data about the persona (e.g., Elon Musk&amp;rsquo;s interviews and tweets) as context, and then tag on your question.</description>
    </item>
    <item>
      <title>Quantization in LLMS (Part 1): LLM.int8(), NF4</title>
      <link>http://localhost:1313/posts/quantization/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/quantization/</guid>
      <description>Introduction to Quantization Whether you&amp;rsquo;re an AI enthusiast looking to run large language models (LLMs) on your personal device, a startup aiming to serve state-of-the-art models efficiently, or a researcher fine-tuning models for specific tasks, quantization is a key technique to understand.
Quantization can be broadly categorized into two main approaches:
Quantization Aware Training (QAT): This involves training the model with reduced precision, allowing it to adjust during the training process to perform well under quantized conditions.</description>
    </item>
    <item>
      <title>Quantization in LLMS Part 2: GPTQ [Draft]</title>
      <link>http://localhost:1313/posts/gptq/</link>
      <pubDate>Thu, 01 Aug 2024 09:51:17 -0400</pubDate>
      <guid>http://localhost:1313/posts/gptq/</guid>
      <description>Introduction
Quantization is a crucial technique in deep learning that reduces the memory footprint and computational requirements of neural networks by representing weights and activations with lower-precision numerical formats. This is particularly important when deploying large models on devices with limited resources. However, quantizing a neural network without significantly degrading its performance is challenging.
The GPTQ (Gradient Post-Training Quantization) algorithm is a method designed to efficiently quantize large-scale neural networks, such as those used in natural language processing, while maintaining high accuracy.</description>
    </item>
  </channel>
</rss>
